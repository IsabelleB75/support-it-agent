# Retraining AutomatisÃ© - Pipeline MLOps

## Vue d'ensemble

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    PIPELINE MLOPS COMPLET                       â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Agent   â”‚â”€â”€â”€>â”‚ Feedback â”‚â”€â”€â”€>â”‚ Retrain  â”‚â”€â”€â”€>â”‚  Deploy  â”‚  â”‚
â”‚  â”‚  RÃ©pond  â”‚    â”‚  Humain  â”‚    â”‚   Auto   â”‚    â”‚   Auto   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚       â”‚                â”‚               â”‚               â”‚        â”‚
â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚                         BOUCLE CONTINUE                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Inference vs Training - Concept clÃ©

### DÃ©finitions

| Concept | DÃ©finition | Le modÃ¨le change ? |
|---------|-----------|-------------------|
| **Training** | Le modÃ¨le apprend des patterns (ajuste ses poids) | OUI |
| **Inference** | Le modÃ¨le utilise ses patterns pour produire un rÃ©sultat | NON |

### Application dans notre projet

Lors du retraining, deux Ã©tapes se succÃ¨dent :

```
Nouveaux textes (prediction_logs avec feedback)
        â”‚
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Sentence Transformers (INFERENCE)          â”‚
â”‚  - Convertit texte â†’ vecteur 384 dim        â”‚
â”‚  - ModÃ¨le PRE-ENTRAINE (ne change pas)      â”‚
â”‚  - Prend ~30 min pour 17000 textes          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  XGBoost (TRAINING)                         â”‚
â”‚  - Apprend Ã  classifier queue/urgence       â”‚
â”‚  - ModÃ¨le ENTRAINE par nous (change)        â”‚
â”‚  - Prend ~1 min                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
        â–¼
Nouveau modÃ¨le dÃ©ployÃ©
```

### Pourquoi re-vectoriser Ã  chaque retraining ?

Les nouvelles questions (via `prediction_logs`) arrivent en **texte brut**. Or XGBoost ne comprend pas le texte - il a besoin de **vecteurs numÃ©riques**.

Donc Ã  chaque retraining :
1. On rÃ©cupÃ¨re le dataset enrichi (ancien + nouveaux feedbacks)
2. On re-vectorise TOUT avec Sentence Transformers (**inference**)
3. On entraÃ®ne XGBoost sur ces vecteurs (**training**)

**C'est l'inference qui prend le plus de temps** (~30 min) car on traite 17000+ textes. Mais le modÃ¨le Sentence Transformers ne change jamais - il "travaille" simplement.

## Ã‰tape 1: Collecte des feedbacks

### Sources de feedback

| Source | Type | Valeur |
|--------|------|--------|
| Bouton ðŸ‘ðŸ‘Ž | Explicite | +1 / -1 |
| Ticket rÃ©ouvert | Implicite | -1 (pas rÃ©solu) |
| Correction agent | Gold | Nouvelle donnÃ©e |
| Temps rÃ©solution | Implicite | Court = bon |

### Interface utilisateur

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Agent: "Voici comment rÃ©soudre votre       â”‚
â”‚          problÃ¨me de VPN..."                â”‚
â”‚                                             â”‚
â”‚  Cette rÃ©ponse vous a-t-elle aidÃ©?          â”‚
â”‚                                             â”‚
â”‚      [ ðŸ‘ Oui ]    [ ðŸ‘Ž Non ]               â”‚
â”‚                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Stockage des feedbacks

```python
# Schema base de donnÃ©es
feedback_table = {
    "id": "uuid",
    "timestamp": "datetime",
    "question": "text",
    "response": "text",
    "category": "string",
    "feedback_score": "int",  # -1, 0, +1
    "agent_correction": "text",  # Si corrigÃ©
    "used_for_training": "bool"  # DÃ©jÃ  utilisÃ©?
}
```

## Ã‰tape 2: Triggers de retraining

### Conditions de dÃ©clenchement

```python
def should_retrain():
    # Condition 1: Assez de nouvelles donnÃ©es
    new_feedbacks = db.count(used_for_training=False)
    if new_feedbacks >= 1000:
        return True, "1000+ nouveaux feedbacks"

    # Condition 2: Performance dÃ©gradÃ©e
    recent_satisfaction = get_satisfaction_last_7_days()
    if recent_satisfaction < 0.80:
        return True, "Satisfaction < 80%"

    # Condition 3: Scheduled (hebdomadaire)
    if is_sunday_night():
        return True, "Retraining hebdomadaire"

    return False, "Pas besoin"
```

### Orchestration avec Airflow

```python
# dag_retrain.py
from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta

default_args = {
    'owner': 'mlops',
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

with DAG(
    'retrain_support_model',
    default_args=default_args,
    schedule_interval='0 2 * * 0',  # Dimanche 2h du matin
    catchup=False,
) as dag:

    check_trigger = PythonOperator(
        task_id='check_retrain_needed',
        python_callable=should_retrain
    )

    prepare_data = PythonOperator(
        task_id='prepare_training_data',
        python_callable=prepare_data_for_training
    )

    train_model = PythonOperator(
        task_id='train_model_rlhf',
        python_callable=run_rlhf_training
    )

    evaluate = PythonOperator(
        task_id='evaluate_new_model',
        python_callable=evaluate_model
    )

    deploy = PythonOperator(
        task_id='deploy_if_better',
        python_callable=deploy_new_model
    )

    check_trigger >> prepare_data >> train_model >> evaluate >> deploy
```

## Ã‰tape 3: PrÃ©paration des donnÃ©es

```python
def prepare_data_for_training():
    # RÃ©cupÃ©rer les nouveaux feedbacks
    new_data = db.query("""
        SELECT question, response, feedback_score, agent_correction
        FROM feedbacks
        WHERE used_for_training = False
        AND feedback_score != 0
    """)

    # CrÃ©er le dataset pour RLHF
    training_data = []

    for row in new_data:
        if row['agent_correction']:
            # Si corrigÃ© par agent = gold standard
            training_data.append({
                'prompt': row['question'],
                'chosen': row['agent_correction'],  # Bonne rÃ©ponse
                'rejected': row['response']  # Mauvaise rÃ©ponse
            })
        else:
            # Sinon utiliser le score
            training_data.append({
                'prompt': row['question'],
                'response': row['response'],
                'reward': row['feedback_score']
            })

    # Sauvegarder
    save_dataset(training_data, 'new_training_data.json')

    # Marquer comme utilisÃ©
    db.update("UPDATE feedbacks SET used_for_training = True WHERE ...")

    return len(training_data)
```

## Ã‰tape 4: Retraining RLHF

```python
def run_rlhf_training():
    from trl import PPOTrainer, PPOConfig

    # Charger le modÃ¨le actuel
    model = load_current_model()
    ref_model = load_reference_model()

    # Charger les nouvelles donnÃ©es
    dataset = load_dataset('new_training_data.json')

    # Config PPO
    config = PPOConfig(
        learning_rate=1e-5,
        batch_size=8,
        mini_batch_size=8,
    )

    # Trainer
    trainer = PPOTrainer(
        config=config,
        model=model,
        ref_model=ref_model,
        tokenizer=tokenizer,
        dataset=dataset,
    )

    # Training loop
    for batch in trainer.dataloader:
        # GÃ©nÃ©rer rÃ©ponses
        responses = trainer.generate(batch['input_ids'])

        # Calculer rewards (depuis le reward model)
        rewards = reward_model(batch['prompt'], responses)

        # Step PPO
        trainer.step(batch['input_ids'], responses, rewards)

    # Sauvegarder
    model.save_pretrained('models/new_model')

    # Log dans MLflow
    mlflow.log_artifact('models/new_model')

    return 'models/new_model'
```

## Ã‰tape 5: Ã‰valuation

```python
def evaluate_model():
    # Charger les modÃ¨les
    new_model = load_model('models/new_model')
    old_model = load_model('models/current_model')

    # Dataset de test
    test_data = load_test_dataset()

    # Ã‰valuer les deux
    new_scores = []
    old_scores = []

    for sample in test_data:
        # GÃ©nÃ©rer rÃ©ponses
        new_response = new_model.generate(sample['prompt'])
        old_response = old_model.generate(sample['prompt'])

        # Scorer avec reward model
        new_scores.append(reward_model(sample['prompt'], new_response))
        old_scores.append(reward_model(sample['prompt'], old_response))

    # Comparer
    new_avg = sum(new_scores) / len(new_scores)
    old_avg = sum(old_scores) / len(old_scores)

    # Log
    mlflow.log_metrics({
        'new_model_score': new_avg,
        'old_model_score': old_avg,
        'improvement': new_avg - old_avg
    })

    return {
        'new_score': new_avg,
        'old_score': old_avg,
        'should_deploy': new_avg > old_avg
    }
```

## Ã‰tape 6: DÃ©ploiement automatique

```python
def deploy_new_model():
    eval_results = get_evaluation_results()

    if not eval_results['should_deploy']:
        log("Nouveau modÃ¨le pas meilleur, on garde l'ancien")
        return False

    # Backup ancien modÃ¨le
    backup_model('models/current_model', 'models/backup/')

    # Remplacer
    copy_model('models/new_model', 'models/current_model')

    # Recharger l'API
    restart_api_server()

    # Notifier
    send_notification(
        f"Nouveau modÃ¨le dÃ©ployÃ©! "
        f"Score: {eval_results['new_score']:.2f} "
        f"(+{eval_results['improvement']:.2f})"
    )

    return True
```

## Outils recommandÃ©s

| Ã‰tape | Outil |
|-------|-------|
| Orchestration | Airflow / Prefect |
| Stockage donnÃ©es | PostgreSQL / MongoDB |
| Training | TRL + HuggingFace |
| Tracking expÃ©riences | MLflow / W&B |
| DÃ©ploiement | Docker + Kubernetes |
| CI/CD | GitHub Actions |
| Monitoring | Prometheus + Grafana |

## Exemple de workflow GitHub Actions

```yaml
# .github/workflows/retrain.yml
name: Retrain Model

on:
  schedule:
    - cron: '0 2 * * 0'  # Dimanche 2h
  workflow_dispatch:  # Manuel aussi

jobs:
  retrain:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'

      - name: Install dependencies
        run: pip install -r requirements.txt

      - name: Check if retrain needed
        run: python scripts/check_retrain.py

      - name: Prepare data
        run: python scripts/prepare_data.py

      - name: Train model
        run: python scripts/train_rlhf.py

      - name: Evaluate
        run: python scripts/evaluate.py

      - name: Deploy if better
        run: python scripts/deploy.py
```

## RÃ©sumÃ©

```
FEEDBACK â†’ TRIGGER â†’ PREPARE â†’ TRAIN â†’ EVALUATE â†’ DEPLOY
    â†‘                                                â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    BOUCLE CONTINUE
```

**C'est Ã§a le MLOps moderne : amÃ©lioration continue automatisÃ©e !**
